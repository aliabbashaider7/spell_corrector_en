{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('party', 'NN')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize(\"We are going to the party\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate, demo_grammar\n",
    "from nltk import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(demo_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the first 23 sentences for demo grammar:\n",
      "\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\n",
      "  1. the man slept\n",
      "  2. the man saw the man\n",
      "  3. the man saw the park\n",
      "  4. the man saw the dog\n",
      "  5. the man saw a man\n",
      "  6. the man saw a park\n",
      "  7. the man saw a dog\n",
      "  8. the man walked in the man\n",
      "  9. the man walked in the park\n",
      " 10. the man walked in the dog\n",
      " 11. the man walked in a man\n",
      " 12. the man walked in a park\n",
      " 13. the man walked in a dog\n",
      " 14. the man walked with the man\n",
      " 15. the man walked with the park\n",
      " 16. the man walked with the dog\n",
      " 17. the man walked with a man\n",
      " 18. the man walked with a park\n",
      " 19. the man walked with a dog\n",
      " 20. the park slept\n",
      " 21. the park saw the man\n",
      " 22. the park saw the park\n",
      " 23. the park saw the dog\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "from nltk.grammar import Nonterminal\n",
    "\n",
    "\n",
    "def generate(grammar, start=None, depth=None, n=None):\n",
    "    \"\"\"\n",
    "    Generates an iterator of all sentences from a CFG.\n",
    "\n",
    "    :param grammar: The Grammar used to generate sentences.\n",
    "    :param start: The Nonterminal from which to start generate sentences.\n",
    "    :param depth: The maximal depth of the generated tree.\n",
    "    :param n: The maximum number of sentences to return.\n",
    "    :return: An iterator of lists of terminal tokens.\n",
    "    \"\"\"\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = sys.maxsize\n",
    "\n",
    "    iter = _generate_all(grammar, [start], depth)\n",
    "\n",
    "    if n:\n",
    "        iter = itertools.islice(iter, n)\n",
    "\n",
    "    return iter\n",
    "\n",
    "\n",
    "\n",
    "def _generate_all(grammar, items, depth):\n",
    "    if items:\n",
    "        try:\n",
    "            for frag1 in _generate_one(grammar, items[0], depth):\n",
    "                for frag2 in _generate_all(grammar, items[1:], depth):\n",
    "                    yield frag1 + frag2\n",
    "        except RuntimeError as _error:\n",
    "            if _error.message == \"maximum recursion depth exceeded\":\n",
    "                # Helpful error message while still showing the recursion stack.\n",
    "                raise RuntimeError(\"The grammar has rule(s) that yield infinite recursion!!\")\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        yield []\n",
    "\n",
    "\n",
    "def _generate_one(grammar, item, depth):\n",
    "    if depth > 0:\n",
    "        if isinstance(item, Nonterminal):\n",
    "            for prod in grammar.productions(lhs=item):\n",
    "                for frag in _generate_all(grammar, prod.rhs(), depth-1):\n",
    "                    yield frag\n",
    "        else:\n",
    "            yield [item]\n",
    "\n",
    "demo_grammar = \"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det N\n",
    "  PP -> P NP\n",
    "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'man' | 'park' | 'dog'\n",
    "  P -> 'in' | 'with'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def demo(N=23):\n",
    "    from nltk.grammar import CFG\n",
    "\n",
    "    print('Generating the first %d sentences for demo grammar:' % (N,))\n",
    "    print(demo_grammar)\n",
    "    grammar = CFG.fromstring(demo_grammar)\n",
    "    for n, sent in enumerate(generate(grammar, n=N), 1):\n",
    "        print('%3d. %s' % (n, ' '.join(sent)))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "prepchoices = nltk.ConditionalFreqDist((v[0], p[0]) \n",
    "    for (v, p) in nltk.bigrams(brown.tagged_words(tagset=\"universal\")) \n",
    "        if v[1] == \"VERB\" and p[1] == \"ADP\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'in': 5, 'at': 3, 'from': 3, 'to': 2, 'on': 1, 'for': 1, 'about': 1, 'since': 1, 'under': 1, 'with': 1})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepchoices[\"writing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {}\n",
    "grammar[\"sitting\"] = {}\n",
    "grammar[\"sitting\"][\"table\"] = \"on\"\n",
    "grammar[\"sitting\"][\"van\"] = \"in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sitting': {'van': 'in', 'table': 'on'}}\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saurav]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "sent = \"when the bell rang, saurav went out\"\n",
    "doc=nlp(sent)\n",
    "\n",
    "sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "\n",
    "print(sub_toks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, world.', 'Here are two sentences.']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "raw_text = 'Hello, world. Here are two sentences.'\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7425985699627899538 the\n",
      "shop 15809682053778148938 shop\n",
      "is 10382539506755952630 be\n",
      "closed 16417442958758597567 close\n",
      ". 12646065887601541794 .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(u\"the shop is closed.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks(doc, drop_determiners=True, min_freq=1):\n",
    "    \"\"\"\n",
    "    Extract an ordered sequence of noun chunks from a spacy-parsed doc, optionally\n",
    "    filtering by frequency and dropping leading determiners.\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc``)\n",
    "        drop_determiners (bool): remove leading determiners (e.g. \"the\")\n",
    "            from phrases (e.g. \"the quick brown fox\" => \"quick brown fox\")\n",
    "        min_freq (int): remove chunks that occur in ``doc`` fewer than\n",
    "            ``min_freq`` times\n",
    "    Yields:\n",
    "        ``spacy.Span``: the next noun chunk from ``doc`` in order of appearance\n",
    "        in the document\n",
    "    \"\"\"\n",
    "    if hasattr(doc, 'spacy_doc'):\n",
    "        ncs = doc.spacy_doc.noun_chunks\n",
    "    else:\n",
    "        ncs = doc.noun_chunks\n",
    "    if drop_determiners is True:\n",
    "        ncs = (nc if nc[0].pos != DET else nc[1:]\n",
    "               for nc in ncs)\n",
    "    if min_freq > 1:\n",
    "        ncs = list(ncs)\n",
    "        freqs = itertoolz.frequencies(nc.lower_ for nc in ncs)\n",
    "        ncs = (nc for nc in ncs\n",
    "               if freqs[nc.lower_] >= min_freq)\n",
    "\n",
    "    for nc in ncs:\n",
    "        yield nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object noun_chunks at 0x7f1477330a98>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_chunks(\"the boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 is                                                  \n",
      "  _______________________________|_________                                           \n",
      " |     |                                   is                                        \n",
      " |     |       ____________________________|________________________                  \n",
      " |     |      |    |   |   |              are             |         |                \n",
      " |     |      |    |   |   |      _________|_______       |         |                 \n",
      " |     |      |    |   |   |     |         |      easy    |         do               \n",
      " |     |      |    |   |   |     |         |       |      |      ___|_____            \n",
      " |  downside  |    |   |   |     |      programs  use    easy   |      analysis      \n",
      " |     |      |    |   |   |     |         |       |      |     |    _____|_______    \n",
      " .    The    that  ,   ,   it because statistical  to  equally  to the          wrong\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "doc = en_nlp(\"The downside is that, because statistical programs are easy to use, it is equally easy to do the wrong analysis.\")\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 plays_ROOT_VBZ               \n",
      "        _______________|______________         \n",
      "       |                          in_prep_IN  \n",
      "       |                              |        \n",
      "       |                        garden_pobj_NN\n",
      "       |                              |        \n",
      "children_nsubj_N                  the_det_DT  \n",
      "       NS                                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "doc = en_nlp(\"children plays in the garden\")\n",
    "\n",
    "def tok_format(tok):\n",
    "    return \"_\".join([tok.orth_, tok.dep_, tok.tag_])\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       swimming_VBG         \n",
      "   _________|__________      \n",
      "  |         |        in_IN  \n",
      "  |         |          |     \n",
      "  |         |       river_NN\n",
      "  |         |          |     \n",
      "He_PRP   was_VBD     the_DT \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = en_nlp(\"He was swimming in the river\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               came_VBD                                                 \n",
      "  ________________________________|______________________________________________        \n",
      " |    |      |     |           got_VBD                    to_IN                  |      \n",
      " |    |      |     |      ________|________                 |                    |       \n",
      " |    |      |     |     |        |     email_NN         house_NN           started_VBD \n",
      " |    |      |     |     |        |        |         _______|_________           |       \n",
      ",_, he_PRP and_CC ._. When_WRB  he_PRP   the_DT  my_PRP$ small_JJ office_NN shouting_VBG\n",
      "\n",
      "came\n",
      "to\n",
      "house\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"When he got the email, he came to my small office house and started shouting.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      walking_VBG            \n",
      "   ________|_____________     \n",
      "  |        |       |   on_IN \n",
      "  |        |       |     |    \n",
      "  |        |       |  road_NN\n",
      "  |        |       |     |    \n",
      "I_PRP    am_VBP   ._.  the_DT\n",
      "\n",
      "walking\n",
      "on\n",
      "road\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"I am walking on the road.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         playing_VBG                             \n",
      "    __________|_____________________________      \n",
      "   |                    |                 in_IN  \n",
      "   |                    |                   |     \n",
      "   |                 boys_NNS           garden_NN\n",
      "   |           _________|_________          |     \n",
      "were_VBD    The_DT            little_JJ   the_DT \n",
      "\n",
      "playing\n",
      "in\n",
      "garden\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"The little boys were playing in the garden\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       found_VBN                                          \n",
      "  _________________________|__________________________________________     \n",
      " |      |         |        |      |                 |               in_IN \n",
      " |      |         |        |      |                 |                 |    \n",
      " |      |         |        |      |            confusion_NN        case_NN\n",
      " |      |         |        |      |       __________|_________        |    \n",
      ",_, Salman_NNP was_VBD guilty_JJ ._. Admist_VB              all_DT  the_DT\n",
      "\n",
      "found\n",
      "in\n",
      "case\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"Admist all confusion, Salman was found guilty in the case.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   cooking_VBG                                                           \n",
      "    ____________________________________|_______________________________________                          \n",
      "   |        |       |        |                     |                       playing_VBG                   \n",
      "   |        |       |        |                     |                  __________|___________________      \n",
      "   |        |       |        |                   in_IN               |          |         |       in_IN  \n",
      "   |        |       |        |                     |                 |          |         |         |     \n",
      "   |        |       |    mother_NN             kitchen_NN            |          |      boys_NNS garden_NN\n",
      "   |        |       |        |           __________|_________        |          |         |         |     \n",
      "was_VBD dinner_NN and_CC   The_DT     the_DT              home_NN were_VBD     ._.      the_DT    the_DT \n",
      "\n",
      "{'cooking': {'kitchen': 'in'}, 'playing': {'garden': 'in'}}\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"The mother was cooking dinner in the home kitchen and the boys were playing in the garden.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "\n",
    "grammar = {}\n",
    "\n",
    "def VB_IN_NN(payload):\n",
    "    if(payload.tag_[:2] != 'VB'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            VB_IN_NN(ch)\n",
    "    temp = [payload]\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            temp.append(ch)\n",
    "            for sec in ch.children:\n",
    "                temp.append(sec)\n",
    "                if(len(temp) == 3):\n",
    "                    grammar[payload.text.lower()] = {}\n",
    "                    grammar[payload.text.lower()][sec.text.lower()] = ch.text.lower()\n",
    "                return\n",
    "    \n",
    "\n",
    "for sent in doc3.sents:\n",
    "    VB_IN_NN(sent.root)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "mdetok = MosesDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly session brought much good\n",
      "\n",
      "The General Assembly, which adjourns today, has performed in an atmosphere of crisis and struggle from the day it convened.\n",
      "\n",
      "It was faced immediately with a showdown on the schools, an issue which was met squarely in conjunction with the governor with a decision not to risk abandoning public education.\n",
      "\n",
      "There followed the historic appropriations and budget fight, in which the General Assembly decided to tackle executive powers.\n",
      "\n",
      "The final decision went to the executive but a way has been opened for strengthening budgeting procedures and to provide legislators information they need.\n",
      "\n",
      "Long-range planning of programs and ways to finance them have become musts if the state in the next few years is to avoid crisis-to-crisis government.\n",
      "\n",
      "This session, for instance, may have insured a financial crisis two years from now.\n",
      "\n",
      "In all the turmoil, some good legislation was passed.\n",
      "\n",
      "Some other good bills were lost in the shuffle and await future action.\n",
      "\n",
      "Certainly all can applaud passage of an auto title law, the school bills, the increase in teacher pensions, the ban on drag racing, acceptance by the state of responsibility for maintenance of state roads in municipalities at the same rate as outside city limits, repeal of the college age limit law and the road maintenance bond issue.\n",
      "\n",
      "No action has been taken, however, on such major problems as ending the fee system, penal reform, modification of the county unit system and in outright banning of fireworks sales.\n",
      "\n",
      "Only a token start was made in attacking the tax reappraisal question and its companion issue of attracting industry to the state.\n",
      "\n",
      "The legislature expended most of its time on the schools and appropriations questions.\n",
      "\n",
      "Fortunately it spared us from the usual spate of silly resolutions which in the past have made Georgia look like anything but \"the empire state of the South\".\n",
      "\n",
      "We congratulate the entire membership on its record of good legislation.\n",
      "\n",
      "In the interim between now and next year, we trust the House and Senate will put their minds to studying Georgia's very real economic, fiscal and social problems and come up with answers without all the political heroics.\n",
      "\n",
      "League regularly stands on the side of right\n",
      "\n",
      "The League of Women Voters, 40 now and admitting it proudly, is inviting financial contributions in the windup of its fund drive.\n",
      "\n",
      "It's a good use of money.\n",
      "\n",
      "These women whose organization grew out of the old suffrage movement are dedicated to Thomas Jefferson's dictum that one must cherish the people's spirit but \"Keep alive their attention\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in brown.sents('cb01')[:20]:\n",
    "    munged_sentence = ' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\")\n",
    "    print(mdetok.detokenize(munged_sentence.split(), return_str=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agreed': {'agenda': 'upon'}, 'send': {'homes': 'to'}, 'propose': {'amount': 'by'}, 'comes': {'walk': 'within'}, 'view': {'conspiracy': 'as'}, 'needed': {'program': 'for'}, 'run': {'nomination': 'for'}, 'rising': {'massachusetts': 'in'}, 'penalized': {'services': 'for'}, 'came': {'ballot': 'on'}, 'credits': {'reduction': 'with'}, 'paying': {'it': 'for'}, 'looks': {'administration': 'to'}, 'alloted': {'municipalities': 'to'}, 'freeze': {'laos': 'in'}, 'warned': {'meeting': 'in'}, 'count': {'aid': 'on'}, 'modernized': {'station': 'with'}, 'arranging': {'graduates': 'for'}, 'boost': {'5,000': 'to'}, 'offer': {'developments': 'in'}, 'saved': {'mitchell': 'for'}, 'passed': {'dissent': 'without'}, 'change': {'insurance': 'to'}, 'back': {'hilt': 'to'}, 'attend': {'portland': 'in'}, 'encourage': {'nations': 'in'}, 'examined': {'session': 'at'}, 'authorized': {'session': 'at'}, 'launched': {'be': 'into'}, 'feeling': {'even': 'for'}, 'support': {'efforts': 'in'}, 'reported': {'day': 'after'}, 'spoke': {'months': 'for'}, 'told': {'luncheon': 'at'}, 'questioned': {'wagner': 'by'}, 'presented': {'court': 'in'}, 'arises': {'moving': 'with'}, 'commented': {'adoption': 'at'}, 'gave': {'time': 'at'}, 'tied': {'message': 'in'}, 'whipped': {'fare': 'through'}, 'use': {'means': 'as'}, 'acclaimed': {'performing': 'for'}, 'post': {'home': 'at'}, 'disclosed': {'davenport': 'by'}, 'announced': {'raymond': 'by'}, 'recorded': {'avenue': 'along'}, 'reside': {'dr': 'at'}, 'work': {'government': 'for'}, 'expected': {'house': 'in'}, 'shot': {'death': 'to'}, 'leaned': {'bench': 'over'}, 'holds': {'struggle': 'without'}, 'called': {'patience': 'for'}, 'knocked': {'of': 'out'}, 'pays': {'workers': 'for'}, 'play': {'determining': 'in'}, 'declined': {'interviews': 'in'}, 'accused': {'spent': 'of'}, 'settle': {'1961': 'during'}, 'amount': {'dollars': 'to'}, 'build': {'decade': 'in'}, 'report': {'treasurer': 'to'}, 'adopt': {'means': 'as'}, 'visiting': {'home': 'at'}, 'powered': {'lines': 'with'}, 'acting': {'pressure': 'under'}, 'faced': {'session': 'at'}, 'approach': {'viewpoint': 'from'}, 'repay': {'period': 'over'}, 'expended': {'funds': 'as'}, 'appoint': {'cooperation': 'with'}, 'provides': {'series': 'for'}, 'went': {'rules': 'under'}, 'employed': {'company': 'by'}, 'formed': {'co.': 'by'}, 'remains': {'leadership': 'of'}, 'hold': {'hotel': 'at'}, 'expanding': {'nations': 'to'}, 'assign': {'section': 'to'}, 'objected': {'tuesday': 'on'}, 'set': {'-': 'by'}, 'compensated': {'services': 'for'}, 'picking': {'rally': 'for'}, 'going': {'medicine': 'into'}, 'sponsored': {'labor': 'by'}, 'be': {'office': 'in'}, 'resolve': {'test': 'on'}, 'were': {'motions': 'of'}, 'urged': {'address': 'in'}, 'stands': {'issue': 'on'}, 'cooking': {'kitchen': 'in'}, 'succeeded': {'ledford': 'by'}, 'concerned': {'powers': 'between'}, 'insist': {'verification': 'on'}, 'dismissed': {'karns': 'by'}, 'turned': {'spring': 'in'}, 'slowed': {'week': 'with'}, 'get': {'wanted': 'to'}, 'aroused': {'year': 'within'}, 'given': {'days': 'during'}, 'listed': {'petition': 'on'}, 'followed': {'forces': 'by'}, 'sending': {'senate': 'to'}, 'think': {'inspiring': 'after'}, 'are': {'springfield': 'in'}, 'reelected': {'term': 'for'}, 'scheduled': {'p.m': 'for'}, 'do': {'cost': 'at'}, 'talked': {'effort': 'in'}, 'call': {'case': 'in'}, 'placed': {'wheel': 'under'}, 'provide': {'loans': 'for'}, 'cited': {'factor': 'as'}, 'attack': {'score': 'on'}, 'elect': {'november': 'in'}, 'learned': {'told': 'after'}, 'vote': {'time': 'for'}, 'witnessed': {'campaign': 'in'}, 'looked': {'laos': 'in'}, 'spent': {'budget': 'in'}, 'designated': {'residential': 'as'}, 'felt': {'face': 'in'}, 'been': {'days': 'in'}, 'retired': {'years': 'after'}, 'pertained': {'eight': 'to'}, 'talking': {'growth': 'of'}, 'described': {'inconclusive': 'as'}, 'is': {'fee': 'with'}, 'absorb': {'termed': 'at'}, 'serving': {'legislature': 'in'}, 'form': {'gursel': 'with'}, 'speak': {'temple': 'at'}, 'met': {'oslo': 'in'}, 'need': {'island': 'in'}, 'jesting': {'mood': 'in'}, 'served': {'june': 'from'}, 'surrounded': {'confusion': 'with'}, 'take': {'developing': 'towards'}, 'asks': {'efforts': 'in'}, 'intervened': {'direction': 'at'}, 'adopted': {'1966': 'in'}, 'gives': {'leaving': 'by'}, 'aided': {'rate': 'at'}, 'stated': {'denying': 'in'}, 'fined': {'hess': 'by'}, 'proposed': {'groups': 'by'}, 'armed': {'pistol': 'with'}, 'laid': {'organization': 'before'}, 'led': {'hearing': 'in'}, 'make': {'ticket': 'in'}, 'collects': {'fees': 'in'}, 'completed': {'months': 'after'}, 'issued': {'assertion': 'in'}, 'throw': {'scramble': 'into'}, 'allocated': {'aid': 'in'}, 'am': {'favor': 'in'}, 'operate': {'basis': 'on'}, 'disagreed': {'value': 'on'}, 'amounts': {'what': 'to'}, 'construed': {'effort': 'as'}, 'supported': {'commissioner': 'by'}, 'moves': {'conference': 'toward'}, 'developed': {'doxiadis': 'by'}, 'encouraging': {'means': 'by'}, 'made': {'berger': 'by'}, 'come': {'loyalist': 'from'}, 'lost': {'needs': 'to'}, 'rescind': {'voting': 'in'}, 'informed': {'done': 'on'}, 'presides': {'bureau': 'over'}, 'claim': {'amount': 'on'}, 'sent': {'senate': 'to'}, 'engaged': {'counseling': 'in'}, 'seemed': {'protests': 'despite'}, 'talk': {'of': 'out'}, 'had': {'election': 'in'}, 'torn': {'months': 'for'}, 'move': {'driveway': 'from'}, 'teach': {'schools': 'in'}, 'ignored': {'order': 'in'}, 'saving': {'coming': 'from'}, 'began': {'afternoon': 'in'}, 'organize': {'aid': 'in'}, 'disable': {'counties': 'in'}, 'pulled': {'party': 'at'}, 'transferred': {'hand': 'from'}, 'devote': {'working': 'to'}, 'rebound': {'discredit': 'to'}, 'imposed': {'those': 'on'}, 'face': {'1961': 'in'}, 'know': {'intervals': 'at'}, 'lacking': {'personnel': 'in'}, 'responding': {'resolution': 'to'}, 'offered': {'association': 'by'}, 'increase': {'1': 'from'}, 'practicing': {'portland': 'in'}, 'doing': {'address': 'on'}, 'recommended': {'matters': 'on'}, 'known': {'it': 'about'}, 'establish': {'counties': 'in'}, 'depends': {'return': 'on'}, 'keeping': {'challenge': 'with'}, 'repeated': {'union': 'to'}, 'voiced': {'committee': 'before'}, 'admitted': {'hearings': 'in'}, 'prevent': {'infiltrating': 'from'}, 'become': {'years': 'after'}, 'comment': {'reforms': 'on'}, 'asking': {'resolution': 'in'}, 'staggered': {'drain': 'by'}, 'done': {'poll': 'by'}, 'printed': {'ballot': 'on'}, 'running': {'candidate': 'against'}, 'drive': {'time': 'during'}, 'prosecute': {'of': 'because'}, 'financed': {'boosting': 'by'}, 'started': {'spokesmen': 'with'}, 'gotten': {'automobile': 'into'}, 'help': {'helping': 'by'}, 'rise': {'7.5': 'to'}, 'greeted': {'chorus': 'with'}, 'writing': {'help': 'with'}, 'goes': {'council': 'to'}, 'promised': {'primary': 'after'}, 'campaign': {'record': 'on'}, 'flow': {'mississippi': 'to'}, 'trim': {'session': 'at'}, 'knew': {'it': 'about'}, 'culminating': {'firm': 'by'}, 'flies': {'house': 'over'}, 'excuses': {'being': 'as'}, 'received': {'talks': 'in'}, 'begin': {'administration': 'with'}, 'joined': {'1925': 'in'}, 'released': {'press': 'to'}, 'let': {'work': 'for'}, 'reduce': {'hours': 'from'}, 'calls': {'speech': 'for'}, 'campaigning': {'carcass': 'on'}, 'obtained': {'procurement': 'for'}, 'decried': {'state': 'in'}, 'fired': {'charges': 'on'}, 'selected': {'plan': 'under'}, 'voted': {'roberts': 'with'}, 'taken': {'present': 'at'}, 'honored': {'colleagues': 'by'}, 'go': {'friday': 'on'}, 'occupying': {'house': 'opposite'}, 'detach': {'bloc': 'from'}, 'ruled': {'cases': 'in'}, 'proselytizing': {'project': 'on'}, 'increased': {'1950': 'since'}, 'paid': {'company': 'to'}, 'distribute': {'departments': 'through'}, 'took': {'project': 'on'}, 'lying': {'driveway': 'on'}, 'cooperate': {'hemphill': 'with'}, 'conditioned': {'prices': 'to'}, 'denied': {'years': 'in'}, 'caused': {'appreciation': 'by'}, 'give': {'issue': 'to'}, 'grooming': {'office': 'for'}, 'starts': {'forecasts': 'with'}, 'place': {'taxpayers': 'on'}, 'attracted': {'climate': 'to'}, 'leaving': {'choice': 'by'}, 'arose': {'martinelli': 'between'}, 'said': {'hearing': 'after'}, 'respond': {'applause': 'with'}, 'rigged': {'starter': 'to'}, 'entail': {'years': 'over'}, 'waiting': {'leadership': 'for'}, 'opposed': {'bid': 'in'}, 'welled': {'leaders': 'among'}, 'according': {'estimates': 'to'}, 'inspired': {'god': 'of'}, 'deal': {'only': 'with'}, 'oppose': {'nomination': 'for'}, 'steered': {'track': 'to'}, 'did': {'entering': 'before'}, 'playing': {'garden': 'in'}, 'plan': {'movement': 'in'}, 'enforced': {'demand': 'by'}, 'viewed': {'evidence': 'as'}, 'subjected': {'calls': 'to'}, 'was': {'hospital': 'in'}, 'charged': {'cases': 'in'}, 'based': {'need': 'on'}, 'pleads': {'administration': 'with'}, 'moving': {'stand': 'past'}, 'carries': {'persons': 'for'}, 'elected': {'advocacy': 'on'}, 'saw': {'program': 'in'}, 'protect': {'fact': 'in'}, 'pay': {'addition': 'in'}, 'visited': {'home': 'at'}, 'reached': {'solution': 'on'}, 'receive': {'majorities': 'by'}, 'defeated': {'voters': 'by'}, 'being': {'polls': 'at'}, 'put': {'test': 'to'}, 'argued': {'behalf': 'in'}, 'have': {'level': 'on'}, 'speaking': {'status': 'of'}, 'proposing': {'changes': 'that'}, 'pass': {'conflict': 'without'}, 'stood': {'revision': 'for'}, 'brought': {'persons': 'against'}, 'declared': {'monday': 'on'}, 'accept': {'suggestion': 'at'}, 'examine': {'revisions': 'with'}, 'incorporated': {'procedures': 'into'}, 'appointed': {'october': 'in'}, 'say': {'much': 'by'}, 'decided': {'vote': 'by'}, 'asked': {'information': 'for'}, 'headed': {'guard': 'by'}, 'raise': {'that': 'after'}, 'opened': {'years': 'in'}, 'mean': {'hotels': 'for'}, 'named': {'president': 'as'}, 'step': {'situation': 'into'}, 'filed': {'day': 'in'}, 'stems': {'fact': 'from'}, 'welcomed': {'delegations': 'by'}, 'sued': {'amount': 'for'}, 'appeared': {'vouchers': 'on'}, 'stalled': {'committee': 'in'}, 'mass': {'thoroughfare': 'along'}, 'emphasizes': {'points': 'in'}, 'bowed': {'pressure': 'to'}, 'follow': {'it': 'on'}, 'studied': {'geneva': 'in'}, 'directed': {'particularly': 'at'}, 'bounded': {'ave': 'by'}, 'revised': {'effort': 'in'}, 'caught': {'squeeze': 'in'}, 'restrain': {'raising': 'from'}, 'carried': {'groups': 'by'}, 'erected': {'avenue': 'along'}, 'credited': {'setting': 'with'}, 'has': {'compromise': 'without'}, 'assailed': {'direction': 'from'}, 'kept': {'committee': 'in'}, 'toss': {'towel': 'in'}, 'start': {'segregation': 'with'}}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for cps in brown.fileids()[:10]:\n",
    "    \n",
    "    for sent in brown.sents(cps):\n",
    "        count += 1\n",
    "        munged_sentence = ' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\")\n",
    "        doc4 = en_nlp(mdetok.detokenize(munged_sentence.split(), return_str=True))\n",
    "        #[to_nltk_tree(sent.root).pretty_print() for sent in doc4.sents]\n",
    "        for sent in doc4.sents:\n",
    "            VB_IN_NN(sent.root)\n",
    "\n",
    "print(grammar)\n",
    "#print(str(len(combos)) + \" \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar[\"joined\"][\"1925\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('correctly.npy', grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_grammar = np.load('correctly.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_IN_NN_correction(payload, raw_text, master_dictionary):\n",
    "\tif(payload.tag_[:2] != 'VB'):\n",
    "\t\treturn\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_[:2] == 'VB'):\n",
    "\t\t\tVB_IN_NN(ch)\n",
    "\ttemp = [payload]\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_ == \"IN\"):\n",
    "\t\t\ttemp.append(ch)\n",
    "\t\t\tfor sec in ch.children:\n",
    "\t\t\t\ttemp.append(sec)\n",
    "\t\t\t\tif(len(temp) == 3):\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tcorrect_prep = master_dictionary[payload.text.lower()][sec.text.lower()]\n",
    "\t\t\t\t\t\tif(correct_prep != ch.text.lower()):\n",
    "\t\t\t\t\t\t\traw_text = raw_text[:ch.idx] + raw_text[ch.idx:].replace(temp[1].text, correct_prep, 1)\n",
    "\t\t\t\t\t\t\treturn raw_text\n",
    "\t\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t\treturn raw_text\n",
    "\t\t\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was dancing with the park.\n"
     ]
    }
   ],
   "source": [
    "text = \"i was dancing with the park.\"\n",
    "doc = en_nlp(text)\n",
    "for sent in doc.sents:\n",
    "    text = VB_IN_NN_correction(sent.root, text, grammar)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import conjugate, lemma, lexeme, INFINITIVE, PRESENT, PAST, PARTICIPLE, FUTURE, SG, PL, INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE, PROGRESSIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading\n"
     ]
    }
   ],
   "source": [
    "print(conjugate(verb='downloading', tense=PRESENT, mood=INDICATIVE, aspect=PROGRESSIVE, person=1, number=PL)) # add aspect=PROGRESSIVE to indicate continuous tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        taken_VBN            \n",
      "    ________|____________     \n",
      "   |        |      |  ball_NN\n",
      "   |        |      |     |    \n",
      "has_VBZ  ram_VBN  ?_.  the_DT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = en_nlp(\"has ram taken the ball?\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               watching_VBG          \n",
      "   _________________|______________   \n",
      "ram_NN has_VBZ   been_VBN   tv_NN ._.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = en_nlp(\"ram has been watching tv.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc2.sents:\n",
    "    for comp in sent.root.children:\n",
    "        if(comp.tag_ == 'VBD'):\n",
    "            print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_VB_VB(payload):\n",
    "    if(payload.tag_[:2] != 'VB'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            VB_VB_VB(ch)\n",
    "    temp = []\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            temp.append(ch.lower_ + '_' + ch.tag_)\n",
    "        if(len(temp) == 2):\n",
    "            temp.append(payload.lower_+ '_' + ch.tag_)\n",
    "            combos.append(temp)\n",
    "            temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramu has been travelling since early this year.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "rtext = \"Ramu has been travel since early this year.\"\n",
    "doc2 = en_nlp(rtext)\n",
    "combos = []\n",
    "for sent in doc2.sents:\n",
    "    rtext = VB_VB_VB_correction(sent.root, rtext)\n",
    "print(rtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_VB_VB_correction(payload, raw_text):\n",
    "    if(payload.tag_[:2] != 'VB' and payload.tag_[:2] != 'NN'  and payload.tag_[:2] != 'JJ'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'): # this might need to be removed\n",
    "            VB_VB_VB_correction(ch, raw_text)\n",
    "    temp = []\n",
    "    nounBeforeVerb = False\n",
    "    nounAfterVerb = False\n",
    "    verbFound = False\n",
    "    since = False\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            verbFound = True\n",
    "        if((not verbFound) and (ch.tag_[:2] == 'NN' or ch.tag_[:2] == 'PR')):\n",
    "            nounBeforeVerb = True\n",
    "        if(verbFound and (ch.tag_[:2] == 'NN' or ch.tag_[:2] == 'PR')):\n",
    "            nounAfterVerb = True\n",
    "        if(ch.lower_ == 'since'):\n",
    "            since = True\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            # print(ch.idx)\n",
    "            temp.append(ch.lower_ + '_' + ch.tag_)\n",
    "        if(len(temp) == 2):\n",
    "            temp.append(payload.lower_+ '_' + ch.tag_)\n",
    "            #print(temp)\n",
    "            if (temp[0][-3:] == 'VBZ' or temp[0][-3:] == 'VBP') and temp[1][-3:] == 'VBN':\n",
    "                if nounAfterVerb or since:\n",
    "                    x = conjugate(verb=lemma(temp[2][:-4]), tense=PRESENT, mood=INDICATIVE, aspect=PROGRESSIVE, person=1, number=PL)\n",
    "                elif nounBeforeVerb:\n",
    "                    x = conjugate(verb=lemma(temp[2][:-4]), tense=PAST+PARTICIPLE, mood=INDICATIVE, person=1, number=PL)\n",
    "                # print(temp[2][:-4] + ' -> ' + x)\n",
    "            combos.append(temp)\n",
    "            # print(nounBeforeVerb)\n",
    "            raw_text = raw_text[:payload.idx] + raw_text[payload.idx:].replace(temp[2][:-4], x, 1)\n",
    "            #print(raw_text)\n",
    "            temp = []\n",
    "            return raw_text\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_IN_NN(payload):\n",
    "\tif(payload.tag_[:2] != 'VB'):\n",
    "\t\treturn\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_[:2] == 'VB'):\n",
    "\t\t\tVB_IN_NN(ch)\n",
    "\ttemp = [payload]\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_ == \"IN\"):\n",
    "\t\t\ttemp.append(ch)\n",
    "\t\t\tfor sec in ch.children:\n",
    "\t\t\t\ttemp.append(sec)\n",
    "\t\t\t\tif(len(temp) == 3):\n",
    "\t\t\t\t\tgrammar[payload.text.lower()] = {}\n",
    "\t\t\t\t\tgrammar[payload.text.lower()][sec.text.lower()] = ch.text.lower()\n",
    "\t\t\t\treturn\n",
    "            \n",
    "def VB_IN_NN_correction(payload, raw_text, master_dictionary):\n",
    "\tif(payload.tag_[:2] != 'VB'):\n",
    "\t\treturn\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_[:2] == 'VB'):\n",
    "\t\t\tVB_IN_NN_correction(ch, raw_text, master_dictionary)\n",
    "\ttemp = [payload]\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_ == \"IN\"):\n",
    "\t\t\ttemp.append(ch)\n",
    "\t\t\tfor sec in ch.children:\n",
    "\t\t\t\ttemp.append(sec)\n",
    "\t\t\t\tif(len(temp) == 3):\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tcorrect_prep = master_dictionary[payload.text.lower()][sec.text.lower()]\n",
    "\t\t\t\t\t\tif(correct_prep != ch.text.lower()):\n",
    "\t\t\t\t\t\t\traw_text = raw_text[:ch.idx] + raw_text[ch.idx:].replace(temp[1].text, correct_prep, 1)\n",
    "\t\t\t\t\t\t\treturn raw_text\n",
    "\t\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t\treturn raw_text\n",
    "\treturn raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_VB_correction(payload, raw_text):\n",
    "    if(payload.tag_[:2] != 'VB'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'): # this might need to be removed\n",
    "            VB_VB_VB_correction(ch, raw_text)\n",
    "            \n",
    "            if(ch.lower_ == 'has') or (ch.lower_ == 'have') or (ch.lower_ == 'had'):\n",
    "                x = conjugate(verb=lemma(payload.text), tense=PAST+PARTICIPLE, mood=INDICATIVE, person=1, number=PL)\n",
    "            else:\n",
    "                x = conjugate(verb=lemma(payload.text), tense=PRESENT, mood=INDICATIVE, aspect=PROGRESSIVE, person=1, number=PL)\n",
    "        \n",
    "            raw_text = raw_text[:payload.idx] + raw_text[payload.idx:].replace(payload.text, x, 1)\n",
    "            return raw_text\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       doing_VBG            \n",
      "   ________|__________       \n",
      "  |        |     homework_NN\n",
      "  |        |          |      \n",
      "he_PRP  has_VBZ    his_PRP$ \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = en_nlp(\"he has doing his homework\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       walking_VBG        \n",
      "   _________|_________     \n",
      "  |         |       on_IN \n",
      "  |         |         |    \n",
      "  |         |      road_NN\n",
      "  |         |         |    \n",
      "he_PRP    is_VBZ    the_DT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = en_nlp(\"he is walking on the road\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He has done his homework.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "rtext = \"He has done his homework.\"\n",
    "doc2 = en_nlp(rtext)\n",
    "combos = []\n",
    "for sent in doc2.sents:\n",
    "    rtext = VB_VB_correction(sent.root, rtext)\n",
    "print(rtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
